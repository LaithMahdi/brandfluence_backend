# compare_recommendation_models.py
import pandas as pd
import numpy as np
import pickle
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import pairwise_distances
import warnings
warnings.filterwarnings('ignore')

# Configuration des graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_prepared_data():
    """Charge les donn√©es pr√©par√©es"""
    print(" Chargement des donn√©es pr√©par√©es...")
    
    df = pd.read_csv('data/influenceurs_recommendation_ready.csv')
    X = np.load('data/feature_matrix.npy')
    
    with open('models/feature_columns.pkl', 'rb') as f:
        feature_columns = pickle.load(f)

    print(f" Donn√©es charg√©es: {len(df)} influenceurs, {X.shape[1]} features")
    return df, X, feature_columns

class Model1_CosineSimilarity:
    """Mod√®le 1: Similarit√© Cosinus"""
    
    def __init__(self, X):
        self.X = X
        self.name = "Cosine Similarity"
        self.similarity_matrix = None
        
    def fit(self):
        """Calcule la matrice de similarit√©"""
        print("üîß Entra√Ænement du mod√®le Cosine Similarity...")
        self.similarity_matrix = cosine_similarity(self.X)
        return self
    
    def recommend(self, query_idx, n=5):
        """Recommande des influenceurs similaires"""
        if self.similarity_matrix is None:
            self.fit()
        
        # Obtenir les indices des plus similaires (exclure lui-m√™me)
        similar_indices = np.argsort(self.similarity_matrix[query_idx])[::-1][1:n+1]
        similarity_scores = self.similarity_matrix[query_idx][similar_indices]
        
        return similar_indices, similarity_scores
    
    def get_model_info(self):
        """Retourne les informations du mod√®le"""
        return {
            'name': self.name,
            'type': 'Similarit√© bas√©e sur le contenu',
            'complexity': 'Faible',
            'speed': 'Rapide (pr√©-calcul√©)',
            'memory': f"Matrice {self.similarity_matrix.shape}",
            'params': 'Aucun hyperparam√®tre'
        }

class Model2_KNN:
    """Mod√®le 2: K-Nearest Neighbors"""
    
    def __init__(self, X, n_neighbors=5):
        self.X = X
        self.n_neighbors = n_neighbors
        self.name = "K-Nearest Neighbors"
        self.model = NearestNeighbors(n_neighbors=n_neighbors+1, 
                                      metric='euclidean', 
                                      algorithm='auto')
        
    def fit(self):
        """Entra√Æne le mod√®le KNN"""
        print(" Entra√Ænement du mod√®le KNN...")
        self.model.fit(self.X)
        return self
    
    def recommend(self, query_idx, n=5):
        """Recommande des influenceurs similaires"""
        if n > self.n_neighbors:
            n = self.n_neighbors
        
        # Reshape pour scikit-learn
        query = self.X[query_idx].reshape(1, -1)
        
        # Trouver les plus proches voisins
        distances, indices = self.model.kneighbors(query, n_neighbors=n+1)
        
        # Exclure le premier (lui-m√™me)
        similar_indices = indices[0][1:n+1]
        similarity_scores = 1 / (1 + distances[0][1:n+1])  # Convertir distance en similarit√©
        
        return similar_indices, similarity_scores
    
    def get_model_info(self):
        """Retourne les informations du mod√®le"""
        return {
            'name': self.name,
            'type': 'M√©thode des k-plus proches voisins',
            'complexity': 'Moyenne',
            'speed': 'Moyenne (recherche √† la vol√©e)',
            'memory': f"Stockage des {len(self.X)} points",
            'params': f'n_neighbors={self.n_neighbors}'
        }

class Model3_ContentBasedFiltering:
    """Mod√®le 3: Filtrage bas√© sur le contenu avec pond√©ration"""
    
    def __init__(self, df, feature_columns):
        self.df = df.copy()
        self.feature_columns = feature_columns
        self.name = "Content-Based Filtering"
        
        # Poids pour diff√©rentes features
        self.weights = {
            'engagement': 0.3,
            'followers': 0.25,
            'category': 0.2,
            'popularity': 0.15,
            'diversity': 0.1
        }
    
    def calculate_similarity(self, query_idx, candidate_idx):
        """Calcule un score de similarit√© personnalis√©"""
        query = self.df.iloc[query_idx]
        candidate = self.df.iloc[candidate_idx]
        
        score = 0
        
        # 1. Similarit√© d'engagement
        engagement_sim = 1 - abs(query['engagement_rate_normalized'] - 
                                candidate['engagement_rate_normalized']) / 2
        score += engagement_sim * self.weights['engagement']
        
        # 2. Similarit√© de followers
        followers_sim = 1 - abs(query['followers_normalized'] - 
                               candidate['followers_normalized']) / 2
        score += followers_sim * self.weights['followers']
        
        # 3. Similarit√© de cat√©gorie
        category_sim = 1 if query['category'] == candidate['category'] else 0.3
        score += category_sim * self.weights['category']
        
        # 4. Popularit√©
        popularity_sim = candidate['global_score']
        score += popularity_sim * self.weights['popularity']
        
        # 5. Diversit√© (p√©nalit√© si m√™me pays)
        diversity_penalty = 0.1 if query['country'] == candidate['country'] else 0
        score -= diversity_penalty * self.weights['diversity']
        
        return min(max(score, 0), 1)  # Normaliser entre 0 et 1
    
    def fit(self):
        """Pr√©pare le mod√®le"""
        print(" Pr√©paration du mod√®le Content-Based...")
        # Pas d'entra√Ænement n√©cessaire pour ce mod√®le simple
        return self
    
    def recommend(self, query_idx, n=5):
        """Recommande des influenceurs"""
        scores = []
        
        for i in range(len(self.df)):
            if i != query_idx:
                score = self.calculate_similarity(query_idx, i)
                scores.append((i, score))
        
        # Trier par score
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # Prendre le top n
        similar_indices = [idx for idx, _ in scores[:n]]
        similarity_scores = [score for _, score in scores[:n]]
        
        return similar_indices, similarity_scores
    
    def get_model_info(self):
        """Retourne les informations du mod√®le"""
        return {
            'name': self.name,
            'type': 'Filtrage bas√© sur contenu avec pond√©ration',
            'complexity': 'Personnalisable',
            'speed': 'Lent (calcul √† la vol√©e)',
            'memory': 'Faible',
            'params': f'weights={self.weights}'
        }

def evaluate_models(models, df, X, n_tests=10):
    """√âvalue les mod√®les sur diff√©rents crit√®res"""
    print("\n" + "="*60)
    print(" √âVALUATION DES MOD√àLES")
    print("="*60)
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\nüîç √âvaluation de: {model_name}")
        
        metrics = {
            'diversity_scores': [],
            'relevance_scores': [],
            'coverage': set(),
            'execution_times': []
        }
        
        import time
        
        # Tester sur plusieurs influenceurs
        test_indices = np.random.choice(len(df), min(n_tests, len(df)), replace=False)
        
        for query_idx in test_indices:
            start_time = time.time()
            
            # Obtenir des recommandations
            recommended_indices, scores = model.recommend(query_idx, n=5)
            
            execution_time = time.time() - start_time
            metrics['execution_times'].append(execution_time)
            
            # 1. Diversit√© (√©viter les recommandations trop similaires entre elles)
            if len(recommended_indices) > 1:
                # Calculer la distance moyenne entre les recommandations
                recommended_features = X[recommended_indices]
                diversity = np.mean(pairwise_distances(recommended_features))
                metrics['diversity_scores'].append(diversity)
            
            # 2. Couverture (combien d'items diff√©rents sont recommand√©s)
            metrics['coverage'].update(recommended_indices)
            
            # 3. Pertinence (simulate avec similarit√© cosinus comme r√©f√©rence)
            reference_scores = cosine_similarity(X[query_idx].reshape(1, -1), 
                                                X[recommended_indices]).flatten()
            relevance = np.mean(reference_scores)
            metrics['relevance_scores'].append(relevance)
        
        # Calculer les moyennes
        results[model_name] = {
            'avg_diversity': np.mean(metrics['diversity_scores']) if metrics['diversity_scores'] else 0,
            'avg_relevance': np.mean(metrics['relevance_scores']),
            'coverage_percentage': len(metrics['coverage']) / len(df) * 100,
            'avg_execution_time': np.mean(metrics['execution_times']),
            'model_info': model.get_model_info()
        }
        
        print(f"  ‚Ä¢ Pertinence moyenne: {results[model_name]['avg_relevance']:.3f}")
        print(f"  ‚Ä¢ Diversit√© moyenne: {results[model_name]['avg_diversity']:.3f}")
        print(f"  ‚Ä¢ Couverture: {results[model_name]['coverage_percentage']:.1f}%")
        print(f"  ‚Ä¢ Temps d'ex√©cution: {results[model_name]['avg_execution_time']:.3f}s")
    
    return results

def visualize_comparison(results, df, models):
    """Visualise la comparaison des mod√®les"""
    print("\n CR√âATION DES VISUALISATIONS...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. Bar plot: Pertinence
    model_names = list(results.keys())
    relevance_scores = [results[m]['avg_relevance'] for m in model_names]
    
    axes[0, 0].bar(model_names, relevance_scores, color='skyblue')
    axes[0, 0].set_title('Pertinence Moyenne des Recommandations')
    axes[0, 0].set_ylabel('Score de Pertinence')
    axes[0, 0].set_ylim([0, 1])
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Ajouter les valeurs sur les barres
    for i, v in enumerate(relevance_scores):
        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')
    
    # 2. Bar plot: Diversit√©
    diversity_scores = [results[m]['avg_diversity'] for m in model_names]
    
    axes[0, 1].bar(model_names, diversity_scores, color='lightgreen')
    axes[0, 1].set_title('Diversit√© Moyenne des Recommandations')
    axes[0, 1].set_ylabel('Score de Diversit√©')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    for i, v in enumerate(diversity_scores):
        axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')
    
    # 3. Bar plot: Couverture
    coverage_scores = [results[m]['coverage_percentage'] for m in model_names]
    
    axes[0, 2].bar(model_names, coverage_scores, color='salmon')
    axes[0, 2].set_title('Couverture des Recommandations')
    axes[0, 2].set_ylabel('Pourcentage de Couverture')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    for i, v in enumerate(coverage_scores):
        axes[0, 2].text(i, v + 0.5, f'{v:.1f}%', ha='center')
    
    # 4. Bar plot: Temps d'ex√©cution
    execution_times = [results[m]['avg_execution_time'] for m in model_names]
    
    axes[1, 0].bar(model_names, execution_times, color='gold')
    axes[1, 0].set_title('Temps d\'Ex√©cution Moyen')
    axes[1, 0].set_ylabel('Secondes')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    for i, v in enumerate(execution_times):
        axes[1, 0].text(i, v + 0.001, f'{v:.3f}s', ha='center')
    
    # 5. Radar chart: Comparaison compl√®te
    ax_radar = axes[1, 1]
    
    # Normaliser les scores pour le radar chart
    categories = ['Pertinence', 'Diversit√©', 'Couverture', 'Vitesse']
    
    # Inverser le temps (plus rapide = mieux)
    max_time = max(execution_times)
    speed_scores = [(max_time - t) / max_time for t in execution_times]
    
    # Pr√©parer les donn√©es
    radar_data = []
    for i, model in enumerate(model_names):
        model_scores = [
            relevance_scores[i],          # Pertinence
            diversity_scores[i] / max(diversity_scores),  # Diversit√© normalis√©e
            coverage_scores[i] / 100,     # Couverture normalis√©e
            speed_scores[i]               # Vitesse normalis√©e
        ]
        radar_data.append(model_scores)
    
    # Cr√©er le radar chart
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # Fermer le cercle
    
    for i, model in enumerate(model_names):
        values = radar_data[i] + radar_data[i][:1]  # Fermer le cercle
        ax_radar.plot(angles, values, 'o-', label=model)
        ax_radar.fill(angles, values, alpha=0.25)
    
    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(categories)
    ax_radar.set_title('Comparaison Radar des Mod√®les')
    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1))
    ax_radar.grid(True)
    
    # 6. Matrice de corr√©lation entre mod√®les
    ax_corr = axes[1, 2]
    
    # Cr√©er une matrice de similarit√© entre les recommandations des mod√®les
    n_influencers = 100  # √âchantillon pour le calcul
    sample_indices = np.random.choice(len(df), min(n_influencers, len(df)), replace=False)
    
    corr_matrix = np.zeros((len(model_names), len(model_names)))
    
    for i, model1 in enumerate(model_names):
        for j, model2 in enumerate(model_names):
            if i <= j:
                # Comparer les recommandations sur l'√©chantillon
                agreements = []
                for idx in sample_indices:
                    rec1, _ = models[model1].recommend(idx, n=3)
                    rec2, _ = models[model2].recommend(idx, n=3)
                    
                    # Calculer le recouvrement
                    overlap = len(set(rec1) & set(rec2)) / 3
                    agreements.append(overlap)
                
                corr_matrix[i, j] = np.mean(agreements)
                corr_matrix[j, i] = corr_matrix[i, j]
    
    # Heatmap
    im = ax_corr.imshow(corr_matrix, cmap='YlOrRd', vmin=0, vmax=1)
    
    # Ajouter les annotations
    for i in range(len(model_names)):
        for j in range(len(model_names)):
            text = ax_corr.text(j, i, f'{corr_matrix[i, j]:.2f}',
                               ha="center", va="center", color="black")
    
    ax_corr.set_xticks(range(len(model_names)))
    ax_corr.set_yticks(range(len(model_names)))
    ax_corr.set_xticklabels([m[:15] for m in model_names], rotation=45)
    ax_corr.set_yticklabels([m[:15] for m in model_names])
    ax_corr.set_title('Corr√©lation entre Mod√®les')
    plt.colorbar(im, ax=ax_corr)
    
    plt.tight_layout()
    plt.savefig('visualizations/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 7. Graphique suppl√©mentaire: Exemple de recommandations
    fig2, ax_example = plt.subplots(figsize=(10, 6))
    
    # Prendre un exemple sp√©cifique
    example_idx = 42
    example_influencer = df.iloc[example_idx]['influencer_name']
    
    recommendations_data = []
    for model_name, model in models.items():
        rec_indices, scores = model.recommend(example_idx, n=3)
        
        for i, (rec_idx, score) in enumerate(zip(rec_indices, scores)):
            rec_name = df.iloc[rec_idx]['influencer_name']
            recommendations_data.append({
                'Model': model_name,
                'Rank': i+1,
                'Influencer': rec_name[:20],
                'Score': score
            })
    
    rec_df = pd.DataFrame(recommendations_data)
    
    # Pivot pour heatmap
    pivot_df = rec_df.pivot(index='Model', columns='Rank', values='Score')
    
    sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='YlGnBu', ax=ax_example)
    ax_example.set_title(f'Recommandations pour: {example_influencer[:30]}...')
    ax_example.set_xlabel('Rang de Recommandation')
    
    plt.tight_layout()
    plt.savefig('visualizations/example_recommendations.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_best_model(results, models):
    """Sauvegarde le meilleur mod√®le"""
    print("\n" + "="*60)
    print("üèÜ S√âLECTION DU MEILLEUR MOD√àLE")
    print("="*60)
    
    # Calculer un score composite
    model_scores = {}
    for model_name, metrics in results.items():
        composite_score = (
            metrics['avg_relevance'] * 0.4 +
            metrics['avg_diversity'] * 0.3 +
            (metrics['coverage_percentage'] / 100) * 0.2 +
            (1 / (1 + metrics['avg_execution_time'])) * 0.1
        )
        model_scores[model_name] = composite_score
    
    # Trier par score
    sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
    
    print("\n SCORES COMPOSITES:")
    for model_name, score in sorted_models:
        print(f"  {model_name}: {score:.3f}")
    
    # S√©lectionner le meilleur
    best_model_name, best_score = sorted_models[0]
    best_model = models[best_model_name]
    
    print(f"\n MEILLEUR MOD√àLE: {best_model_name} (score: {best_score:.3f})")
    
    # Sauvegarder le meilleur mod√®le
    print(f"üíæ Sauvegarde du mod√®le: {best_model_name}")
    
    with open(f'models/best_model_{best_model_name.replace(" ", "_").lower()}.pkl', 'wb') as f:
        pickle.dump(best_model, f)
    
    # Sauvegarder les r√©sultats de comparaison
    comparison_results = {
        'best_model': best_model_name,
        'best_score': best_score,
        'all_scores': model_scores,
        'detailed_results': results,
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    with open('models/model_comparison_results.pkl', 'wb') as f:
        pickle.dump(comparison_results, f)
    
    # Exporter en JSON pour lecture facile
    import json
    
    # Convertir en format JSON-friendly
    json_results = {}
    for model_name, metrics in results.items():
        json_results[model_name] = {
            'avg_relevance': float(metrics['avg_relevance']),
            'avg_diversity': float(metrics['avg_diversity']),
            'coverage_percentage': float(metrics['coverage_percentage']),
            'avg_execution_time': float(metrics['avg_execution_time']),
            'composite_score': float(model_scores[model_name])
        }
    
    with open('models/model_comparison_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'best_model': best_model_name,
            'best_score': float(best_score),
            'model_scores': json_results,
            'recommendation': f"Utiliser {best_model_name} pour votre syst√®me de recommandation"
        }, f, indent=2, ensure_ascii=False)
    
    print("\n FICHIERS CR√â√âS:")
    print(f"  models/best_model_{best_model_name.replace(' ', '_').lower()}.pkl")
    print("  models/model_comparison_results.pkl")
    print("  models/model_comparison_results.json")
    print("  visualizations/model_comparison.png")
    print("  visualizations/example_recommendations.png")
    
    return best_model_name, best_model

def main():
    """Fonction principale"""
    print(" COMPARAISON DE 3 MOD√àLES DE RECOMMANDATION")
    print("="*60)
    
    # Cr√©er le dossier visualizations
    import os
    os.makedirs('visualizations', exist_ok=True)
    
    # 1. Charger les donn√©es
    df, X, feature_columns = load_prepared_data()
    
    # 2. Initialiser les mod√®les
    print("\n INITIALISATION DES 3 MOD√àLES:")
    
    models = {
        'Cosine Similarity': Model1_CosineSimilarity(X).fit(),
        'K-Nearest Neighbors': Model2_KNN(X, n_neighbors=10).fit(),
        'Content-Based Filtering': Model3_ContentBasedFiltering(df, feature_columns).fit()
    }
    
    print(" 3 mod√®les initialis√©s et entra√Æn√©s")
    
    # 3. √âvaluer les mod√®les
    results = evaluate_models(models, df, X, n_tests=20)
    
    # 4. Visualiser la comparaison
    visualize_comparison(results, df, models)  # CORRECTION ICI
    
    # 5. Sauvegarder le meilleur mod√®le
    best_model_name, best_model = save_best_model(results, models)
    
    print("\n" + "="*60)
    print(" COMPARAISON TERMIN√âE !")
    print("="*60)
    print(f"\n MOD√àLE RECOMMAND√â: {best_model_name}")
    print("\n CARACT√âRISTIQUES:")
    model_info = results[best_model_name]['model_info']
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    print("\n PROCHAINES √âTAPES:")
    print("  1. Utiliser le mod√®le sauvegard√© pour votre API")
    print("  2. Tester avec des requ√™tes r√©elles")
    print("  3. Collecter du feedback pour am√©lioration")
    print("  4. Mettre en production l'API de recommandation")

if __name__ == "__main__":
    main()